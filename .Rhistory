train.dat.class <- train.dat
test.dat.class <- test.dat
ctree1 <- tree(recidivate ~ ., data = train.dat.class)
library(tree)
dat <- read.csv("asylum_conjoint_data.csv")
head(dat)
set.seed(123)
n <- nrow(dat)
v <- sample(n,3000,replace = FALSE)
train.dat <- dat[v,]
test.dat <- dat[-v,]
rtree1 <- tree(NAsySeekAccepted ~ ., data = train.dat)
summary(rtree1)
plot(rtree1)
text(rtree1,pretty = 0)
rtree1
rtree2 <- tree(NAsySeekAccepted ~ ., data = train.dat, mindev = 0.002)
summary(rtree2)
plot(rtree2)
rtree2
rtree3 <- tree(NAsySeekAccepted ~ ., data = train.dat, mindev = 0)
summary(rtree3)
plot(rtree3)
rtest1 <- predict(rtree1, newdata = test.dat, type = "vector")
rtest2 <- predict(rtree2, newdata = test.dat, type = "vector")
rtest3 <- predict(rtree3, newdata = test.dat, type = "vector")
mean((test.dat$NAsySeekAccepted - rtest1)^2)
mean((test.dat$NAsySeekAccepted - rtest2)^2)
mean((test.dat$NAsySeekAccepted - rtest3)^2)
train.dat.class <- train.dat
test.dat.class <- test.dat
train.dat.class$NAsySeekAccepted <-
as.factor(train.dat.class$NAsySeekAccepted > 5)
test.dat.class$NAsySeekAccepted <-
as.factor(test.dat.class$NAsySeekAccepted > 5)
names(train.dat.class)[1] <- names(test.dat.class)[1] <-
"MajorityAccepted"
head(train.dat.class)
View(dat)
source('C:/Users/james/Desktop/UCSD/POLI175/DT1/classification_tree.R', echo=TRUE)
library(tree)
rd <- read.csv("recidivism_data_sample.csv")
head(rd)
summary(rd)
# Regression Tree ---------------------------------------------------------
# Data split --------------------------------------------------------------
set.seed(123)
n <- nrow(rd)
v <- sample(n,4000,replace = FALSE)
train.dat <- rd[v,]
test.dat <- rd[-v,]
rtree1 <- tree(recidivate ~ ., data = train.dat)
summary(rtree1)
plot(rtree1)
text(rtree1,pretty = 33)
rtree1
rtree2 <- tree(recidivate ~ ., data = train.dat, mindev = 0.002)
summary(rtree2)
plot(rtree2)
rtree2
rtree3 <- tree(recidivate ~ ., data = train.dat, mindev = 0.003)
summary(rtree3)
plot(rtree3)
text(rtree3)
rtest1 <- predict(rtree1, newdata = test.dat, type = "vector")
rtest2 <- predict(rtree2, newdata = test.dat, type = "vector")
rtest3 <- predict(rtree3, newdata = test.dat, type = "vector")
mean((test.dat$recidivate - rtest1)^2)
mean((test.dat$recidivate - rtest2)^2)
mean((test.dat$recidivate - rtest3)^2)
View(test.dat)
ctest1 <- predict(ctree1, newdata = test.dat.class, type = "class")
ctest1 <- predict(ctree1, newdata = test.dat, type = "class")
ctest2 <- predict(ctree2, newdata = test.dat, type = "class")
ctest3 <- predict(ctree3, newdata = test.dat, type = "class")
ctest1 <- predict(rtree1, newdata = test.dat, type = "class")
train.dat.class <- train.dat
test.dat.class <- test.dat
train.dat.class$recidivate <-
as.factor(train.dat.class$recidivate)
test.dat.class$recidivate <-
as.factor(test.dat.class$recidivate)
head(train.dat.class)
ctest1 <- predict(ctree1, newdata = test.dat.class, type = "class")
ctree1 <- tree(recidivate ~ ., data = train.dat.class)
summary(ctree1)
plot(ctree1)
text(ctree1,pretty = 0)
ctree2 <- tree(recidivate ~ ., data = train.dat.class, mindev = 0.002)
summary(ctree2)
plot(ctree2)
ctree3 <- tree(recidivate ~ ., data = train.dat.class, mindev = 0)
summary(ctree3)
plot(ctree3)
ctest1.predprobs <- predict(ctree1, newdata = test.dat.class, type = "vector")
head(ctest1.predprobs)
ctest1 <- predict(ctree1, newdata = test.dat.class, type = "class")
ctest2 <- predict(ctree2, newdata = test.dat.class, type = "class")
ctest3 <- predict(ctree3, newdata = test.dat.class, type = "class")
mean(test.dat.class$recidivate != ctest1)
mean(test.dat.class$recidivate != ctest2)
mean(test.dat.class$recidivate != ctest3)
class(test.dat$recidivate)
class(train.dat.class$recidivate)
ctree3 <- tree(recidivate ~ ., data = train.dat.class, mindev = 0.003)
summary(ctree3)
plot(ctree3)
ctest1 <- predict(ctree1, newdata = test.dat.class, type = "class")
ctest2 <- predict(ctree2, newdata = test.dat.class, type = "class")
ctest3 <- predict(ctree3, newdata = test.dat.class, type = "class")
mean(test.dat.class$recidivate != ctest1)
mean(test.dat.class$recidivate != ctest2)
mean(test.dat.class$recidivate != ctest3)
ctree3 <- tree(recidivate ~ ., data = train.dat.class, mindev = 0.01)
summary(ctree3)
plot(ctree3)
ctest3 <- predict(ctree3, newdata = test.dat.class, type = "class")
mean(test.dat.class$recidivate != ctest3)
ctree1 <- tree(recidivate ~ ., data = train.dat.class)
summary(ctree1)
plot(ctree1)
text(ctree1,pretty = 0)
ctree2 <- tree(recidivate ~ ., data = train.dat.class, mindev = 0.002)
summary(ctree2)
plot(ctree2)
ctree3 <- tree(recidivate ~ ., data = train.dat.class, mindev = 0)
summary(ctree3)
plot(ctree3)
ctest1.predprobs <- predict(ctree1, newdata = test.dat.class, type = "vector")
head(ctest1.predprobs)
ctest1 <- predict(ctree1, newdata = test.dat.class, type = "class")
ctest2 <- predict(ctree2, newdata = test.dat.class, type = "class")
ctest3 <- predict(ctree3, newdata = test.dat.class, type = "class")
mean(test.dat.class$recidivate != ctest1)
mean(test.dat.class$recidivate != ctest2)
mean(test.dat.class$recidivate != ctest3)
# random forest -----------------------------------------------------------
#    application of bagging to CART but with an additional layer
library(randomForest)
# Data split --------------------------------------------------------------
set.seed(1234)
n <- nrow(rd)
v <- sample(n,4000,replace = FALSE)
train.dat <- rd[v,]
test.dat <- rd[-v,]
# random forest -----------------------------------------------------------
#    application of bagging to CART but with an additional layer
library(randomForest)
# Data split --------------------------------------------------------------
set.seed(1234)
n <- nrow(rd)
v <- sample(n,4000,replace = FALSE)
# random forest -----------------------------------------------------------
#    application of bagging to CART but with an additional layer
library(randomForest)
rd <- read.csv("recidivism_data_sample.csv")
head(rd)
summary(rd)
# Data split --------------------------------------------------------------
set.seed(1234)
n <- nrow(rd)
v <- sample(n,4000,replace = FALSE)
train.dat <- rd[v,]
test.dat <- rd[-v,]
?wdf
train.dat <- as.factor(rd[v,])
set.seed(1234)
n <- nrow(rd)
v <- sample(n,4000,replace = FALSE)
train.dat <- as.factor(rd[v,])
test.dat <- as.factor(rd[-v,])
train.dat <- rd[v,]
test.dat <- rd[-v,]
train.dat.class$recidivate <-
as.factor(train.dat.class$recidivate)
test.dat.class$recidivate <-
as.factor(test.dat.class$recidivate)
train.dat.class <- train.dat
test.dat.class <- test.dat
train.dat.class$recidivate <-
as.factor(train.dat.class$recidivate)
test.dat.class$recidivate <-
as.factor(test.dat.class$recidivate)
#Regression model
set.seed(207)
rf.mod <- randomForest(recidivate ~ ., train.dat, ntree = 1000)
predict(rf.mod, newdata = test.dat)
predict(rf.mod, newdata = test.dat)
#Regression model
set.seed(3956)
rf.mod <- randomForest(recidivate ~ ., train.dat, ntree = 1000)
predict(rf.mod, newdata = test.dat)
summary(rf.mod)
rf.mod$mtry #This is the number of variables randomly considered per tree
rf.mod$mse #This is the out-of-bag MSE at each stage of the model
plot(x = 1:1000, y = rf.mod$mse)
rf.mod$mtry #This is the number of variables randomly considered per tree
View(rf.mod)
#Classification model
str(train.dat.class)
#Classification model
set.seed(3956)
rf.mod.class <- randomForest(recidivate ~ ., train.dat.class,
ntree = 1000)
predict(rf.mod.class, newdata = test.dat.class, type = "response")
predict(rf.mod.class, newdata = test.dat.class, type = "prob")[1:10,]
rf.mod.class$mtry
rf.mod.class$err.rate[1:10,] #OOB is the overall error rate
plot(x = 1:1000, y = rf.mod.class$err.rate[,1])
rf.mod.class <- randomForest(recidivate ~ ., train.dat.class, ntree = 10000)
predict(rf.mod.class, newdata = test.dat.class, type = "response")
predict(rf.mod.class, newdata = test.dat.class, type = "prob")[1:10,]
rf.mod.class$mtry
rf.mod.class$err.rate[1:10,] #OOB is the overall error rate
plot(x = 1:10000, y = rf.mod.class$err.rate[,1])
rf.mod.class$err.rate[1:10,] #OOB is the overall error rate
plot(x = 1:1000, y = rf.mod.class$err.rate[,1])
rf.mod.class$mtry
rf.mod.class$err.rate[1:10,] #OOB is the overall error rate
plot(x = 1:10000, y = rf.mod.class$err.rate[,1])
# Random Forest "CV" ------------------------------------------------------
n.vars <- c(1,2,3,4,5,6,7,8,9,10)
rf.models <- list()
# Random Forest "CV" ------------------------------------------------------
n.vars <- c(1,2,3,4,5,6,7,8,9,10)
rf.models <- list()
#CV to choose m (number of variables to consider at each split)
set.seed(207)
#CV to choose m (number of variables to consider at each split)
set.seed(3956)
for (i in 1:length(n.vars)){
rf.models[[i]] <- randomForest(recidivate ~ .,
train.dat, ntree = 1000,
mtry = n.vars[i])
print(i)
}
n.mods <- length(rf.models)
oob.mse <- rep(NA,n.mods)
n.vars <- rep(NA,n.mods)
for (i in 1:n.mods){
oob.mse[i] <- min(rf.models[[i]]$mse)
n.vars[i] <- rf.models[[i]]$mtry
}
best.mod <- which.min(oob.mse)
n.vars[best.mod]
test.dat.rfpreds <- predict(rf.models[[best.mod]], newdata = test.dat)
#Test set MSE:
mean((test.dat.rfpreds - test.dat$NAsySeekAccepted)^2)
#Test set MSE:
mean((test.dat.rfpreds - test.dat$recidivate)^2)
library(tree)
rd <- read.csv("recidivism_data_sample.csv")
head(rd)
summary(rd)
# Regression Tree ---------------------------------------------------------
# Data split --------------------------------------------------------------
set.seed(123)
n <- nrow(rd)
v <- sample(n,4000,replace = FALSE)
train.dat <- rd[v,]
test.dat <- rd[-v,]
rtree1 <- tree(recidivate ~ ., data = train.dat)
summary(rtree1)
plot(rtree1)
text(rtree1,pretty = 33)
rtree1
rtree2 <- tree(recidivate ~ ., data = train.dat, mindev = 0.002)
summary(rtree2)
plot(rtree2)
rtree2
rtree3 <- tree(recidivate ~ ., data = train.dat, mindev = 0.003)
summary(rtree3)
plot(rtree3)
text(rtree3)
rtest1 <- predict(rtree1, newdata = test.dat, type = "vector")
rtest2 <- predict(rtree2, newdata = test.dat, type = "vector")
rtest3 <- predict(rtree3, newdata = test.dat, type = "vector")
mean((test.dat$recidivate - rtest1)^2)
mean((test.dat$recidivate - rtest2)^2)
mean((test.dat$recidivate - rtest3)^2)
# Classification trees ----------------------------------------------------
train.dat.class <- train.dat
test.dat.class <- test.dat
train.dat.class$recidivate <-
as.factor(train.dat.class$recidivate)
test.dat.class$recidivate <-
as.factor(test.dat.class$recidivate)
head(train.dat.class)
ctree1 <- tree(recidivate ~ ., data = train.dat.class)
summary(ctree1)
plot(ctree1)
text(ctree1,pretty = 0)
ctree2 <- tree(recidivate ~ ., data = train.dat.class, mindev = 0.002)
summary(ctree2)
plot(ctree2)
ctree3 <- tree(recidivate ~ ., data = train.dat.class, mindev = 0)
summary(ctree3)
plot(ctree3)
ctest1.predprobs <- predict(ctree1, newdata = test.dat.class, type = "vector")
head(ctest1.predprobs)
ctest1 <- predict(ctree1, newdata = test.dat.class, type = "class")
ctest2 <- predict(ctree2, newdata = test.dat.class, type = "class")
ctest3 <- predict(ctree3, newdata = test.dat.class, type = "class")
mean(test.dat.class$recidivate != ctest1)
mean(test.dat.class$recidivate != ctest2)
mean(test.dat.class$recidivate != ctest3)
#Test set MSE:
mean((test.dat.rfpreds - test.dat$recidivate)^2)
save(rf.models, file = "rf_cv_models.Rdata")
# Inspect results ---------------------------------------------------------
load("rf_cv_models.Rdata")
rm(list = ls())
library(gbm)
rd <- read.csv("recidivism_data_sample.csv")
head(rd)
# Data split --------------------------------------------------------------
set.seed(1234)
n <- nrow(rd)
v <- sample(n,4000,replace = FALSE)
train.dat <- rd[v,]
test.dat <- rd[-v,]
train.dat.class <- train.dat
test.dat.class <- test.dat
train.dat.class$recidivate <-
as.factor(train.dat.class$recidivate)
test.dat.class$recidivate <-
as.factor(test.dat.class$recidivate)
#Specify distribution = "gaussian" for regression
set.seed(3956)
#Specify distribution = "gaussian" for regression
set.seed(3956)
btmod <- gbm(recidivate ~  ., data = train.dat, shrinkage = 0.01,
distribution = "gaussian", n.trees = 2000,
interaction.depth = 1)
View(rd)
View(rd)
head(rd)
rd <- rd[-1, ]
head(rd)
rd <- read.csv("recidivism_data_sample.csv")
head(rd)
rd <- rd[,-1]
head(rd)
# random forest -----------------------------------------------------------
#    application of bagging to CART but with an additional layer
library(randomForest)
rd <- read.csv("recidivism_data_sample.csv")
head(rd)
#drop ID because it's meaningless
rd <- rd[,-1]
# Data split --------------------------------------------------------------
set.seed(1234)
n <- nrow(rd)
v <- sample(n,4000,replace = FALSE)
train.dat <- rd[v,]
test.dat <- rd[-v,]
train.dat.class <- train.dat
test.dat.class <- test.dat
train.dat.class$recidivate <-
as.factor(train.dat.class$recidivate)
test.dat.class$recidivate <-
as.factor(test.dat.class$recidivate)
# Random Forest Model -----------------------------------------------------
#Regression model
set.seed(3956)
rf.mod <- randomForest(recidivate ~ ., train.dat, ntree = 1000)
predict(rf.mod, newdata = test.dat)
summary(rf.mod)
rf.mod$mtry #This is the number of variables randomly considered per tree
rf.mod$mse #This is the out-of-bag MSE at each stage of the model
plot(x = 1:1000, y = rf.mod$mse)
#Classification model
set.seed(3956)
rf.mod.class <- randomForest(recidivate ~ ., train.dat.class, ntree = 1000)
predict(rf.mod.class, newdata = test.dat.class, type = "response")
predict(rf.mod.class, newdata = test.dat.class, type = "prob")[1:10,]
rf.mod.class$mtry
rf.mod.class$err.rate[1:10,] #OOB is the overall error rate
plot(x = 1:1000, y = rf.mod.class$err.rate[,1])
# Random Forest "CV" ------------------------------------------------------
n.vars <- c(1,2,3,4,5,6,7,8)
rf.models <- list()
#CV to choose m (number of variables to consider at each split)
set.seed(3956)
for (i in 1:length(n.vars)){
rf.models[[i]] <- randomForest(recidivate ~ .,
train.dat, ntree = 1000,
mtry = n.vars[i])
print(i)
}
save(rf.models, file = "rf_cv_models.Rdata")
# Inspect results ---------------------------------------------------------
load("rf_cv_models.Rdata")
n.mods <- length(rf.models)
oob.mse <- rep(NA,n.mods)
n.vars <- rep(NA,n.mods)
for (i in 1:n.mods){
oob.mse[i] <- min(rf.models[[i]]$mse)
n.vars[i] <- rf.models[[i]]$mtry
}
best.mod <- which.min(oob.mse)
n.vars[best.mod]
# Use final model ---------------------------------------------------
test.dat.rfpreds <- predict(rf.models[[best.mod]], newdata = test.dat)
#Test set MSE:
mean((test.dat.rfpreds - test.dat$recidivate)^2)
# boosted tree
rm(list = ls())
library(gbm)
rd <- read.csv("recidivism_data_sample.csv")
head(rd)
#drop ID because it's meaningless
rd <- rd[,-1]
# Data split --------------------------------------------------------------
set.seed(1234)
n <- nrow(rd)
v <- sample(n,4000,replace = FALSE)
train.dat <- rd[v,]
test.dat <- rd[-v,]
train.dat.class <- train.dat
test.dat.class <- test.dat
train.dat.class$recidivate <-
as.factor(train.dat.class$recidivate)
test.dat.class$recidivate <-
as.factor(test.dat.class$recidivate)
btmod.class <- gbm(recidivate ~  ., data = train.dat.class, shrinkage = 0.01,
distribution = "bernoulli", n.trees = 2000,
interaction.depth = 1)
btmod.class$train.error #Again, this is not a valid proxy for test error!
predict(btmod.class, newdata = test.dat.class)
predict(btmod.class, newdata = test.dat.class, n.trees = 1500)
predict(btmod.class, newdata = test.dat.class, n.trees = 1500, type = "response")
#Implementing CV over two parameters: depths and number of trees
depths <- c(1,2,3,4,5,6,7,8,9)
bt.models <- list()
predict(btmod.class, newdata = test.dat.class, n.trees = 1500, type = "response")
View(test.dat)
btmod.class <- gbm(recidivate ~  ., data = train.dat.class, shrinkage = 0.01,
distribution = "bernoulli", n.trees = 2000,
interaction.depth = 1)
btmod.class$train.error #Again, this is not a valid proxy for test error!
btmod.class <- gbm(recidivate ~  ., data = train.dat, shrinkage = 0.01,
distribution = "bernoulli", n.trees = 2000,
interaction.depth = 1)
btmod.class$train.error #Again, this is not a valid proxy for test error!
predict(btmod.class, newdata = test.dat.class)
predict(btmod.class, newdata = test.dat.class, n.trees = 1500)
predict(btmod.class, newdata = test.dat.class, n.trees = 1500, type = "response")
predict(btmod.class, newdata = test.dat)
.class
predict(btmod.class, newdata = test.dat.class)
# boosted tree
rm(list = ls())
library(gbm)
rd <- read.csv("recidivism_data_sample.csv")
head(rd)
#drop ID because it's meaningless
rd <- rd[,-1]
# Data split --------------------------------------------------------------
set.seed(1234)
n <- nrow(rd)
v <- sample(n,4000,replace = FALSE)
train.dat <- rd[v,]
test.dat <- rd[-v,]
# Boosted Trees Model -----------------------------------------------------
btmod.class <- gbm(recidivate ~  ., data = train.dat, shrinkage = 0.01,
distribution = "bernoulli", n.trees = 2000,
interaction.depth = 1)
btmod.class$train.error #Again, this is not a valid proxy for test error!
predict(btmod.class, newdata = test.dat)
predict(btmod.class, newdata = test.dat, n.trees = 1500)
predict(btmod.class, newdata = test.dat, n.trees = 1500, type = "response")
#Implementing CV over two parameters: depths and number of trees
depths <- c(1,2,3,4,5,6,7,8)
bt.models <- list()
set.seed(12345)
for (i in 1:length(depths)){
bt.models[[i]] <- gbm(recidivate ~  .,
data = train.dat, shrinkage = 0.01,
distribution="gaussian", n.trees = 2000,
interaction.depth = depths[i],
cv.folds = 10)
print(i)
}
save(bt.models, file = "bt_cv_models.Rdata")
load("bt_cv_models.Rdata")
bt.one <- bt.models[[5]]
bt.one$n.trees
bt.one$interaction.depth
bt.one$cv.error
plot(x = 1:bt.one$n.trees, y = bt.one$cv.error)
n.depths <- length(bt.models)
depths <- rep(NA, n.depths)
min.cv.error <- rep(NA, n.depths)
best.n.trees <- rep(NA, n.depths)
for (i in 1:n.depths){
bt.curr <- bt.models[[i]]
depths[i] <- bt.curr$interaction.depth
min.cv.error[i] <- min(bt.curr$cv.error)
best.n.trees[i] <- which.min(bt.curr$cv.error)
rm(bt.curr)
}
min.cv.error
m <- which.min(min.cv.error)
final.ntrees <- best.n.trees[m]
final.ntrees
final.depth <- depths[m]
final.depth
test.dat.preds <- predict(bt.models[[m]], newdata = test.dat,
n.trees = final.ntrees)
#Test set MSE:
mean((test.dat.preds - test.dat$recidivate)^2)
