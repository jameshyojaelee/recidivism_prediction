predict(btmod.class, newdata = test.dat.class)
predict(btmod.class, newdata = test.dat.class, n.trees = 1500)
predict(btmod.class, newdata = test.dat.class, n.trees = 1500, type = "response")
predict(btmod.class, newdata = test.dat)
.class
predict(btmod.class, newdata = test.dat.class)
# boosted tree
rm(list = ls())
library(gbm)
rd <- read.csv("recidivism_data_sample.csv")
head(rd)
#drop ID because it's meaningless
rd <- rd[,-1]
# Data split --------------------------------------------------------------
set.seed(1234)
n <- nrow(rd)
v <- sample(n,4000,replace = FALSE)
train.dat <- rd[v,]
test.dat <- rd[-v,]
# Boosted Trees Model -----------------------------------------------------
btmod.class <- gbm(recidivate ~  ., data = train.dat, shrinkage = 0.01,
distribution = "bernoulli", n.trees = 2000,
interaction.depth = 1)
btmod.class$train.error #Again, this is not a valid proxy for test error!
predict(btmod.class, newdata = test.dat)
predict(btmod.class, newdata = test.dat, n.trees = 1500)
predict(btmod.class, newdata = test.dat, n.trees = 1500, type = "response")
#Implementing CV over two parameters: depths and number of trees
depths <- c(1,2,3,4,5,6,7,8)
bt.models <- list()
set.seed(12345)
for (i in 1:length(depths)){
bt.models[[i]] <- gbm(recidivate ~  .,
data = train.dat, shrinkage = 0.01,
distribution="gaussian", n.trees = 2000,
interaction.depth = depths[i],
cv.folds = 10)
print(i)
}
save(bt.models, file = "bt_cv_models.Rdata")
load("bt_cv_models.Rdata")
bt.one <- bt.models[[5]]
bt.one$n.trees
bt.one$interaction.depth
bt.one$cv.error
plot(x = 1:bt.one$n.trees, y = bt.one$cv.error)
n.depths <- length(bt.models)
depths <- rep(NA, n.depths)
min.cv.error <- rep(NA, n.depths)
best.n.trees <- rep(NA, n.depths)
for (i in 1:n.depths){
bt.curr <- bt.models[[i]]
depths[i] <- bt.curr$interaction.depth
min.cv.error[i] <- min(bt.curr$cv.error)
best.n.trees[i] <- which.min(bt.curr$cv.error)
rm(bt.curr)
}
min.cv.error
m <- which.min(min.cv.error)
final.ntrees <- best.n.trees[m]
final.ntrees
final.depth <- depths[m]
final.depth
test.dat.preds <- predict(bt.models[[m]], newdata = test.dat,
n.trees = final.ntrees)
#Test set MSE:
mean((test.dat.preds - test.dat$recidivate)^2)
rm(list = ls())
library(tidyr)
library(glmnet)
dat <- read.csv("recidivism_data_sample.csv")
# Split into predictor vector (y) and feature matrix (x)
y <- dat$recidivate
# =====================
# CREATING THE X MATRIX
# =====================
# OPTION 1: Include all variables
# x <- subset(dat, select = -c(recidivate))
#x <- model.matrix(~.-1, x[,]) # Convert all categorical variable to dummies.
# OPTION 2: Exclude nominal data (id & race), include charge_name as dummies.
# x <- subset(dat, select = -c(recidivate, id, race))
# x <- model.matrix(~.-1, x[,]) # Convert all categorical variable to dummies.
# OPTION 3: Exclude nominal and charge_name (id,race, & charge_name)
x <- as.matrix(subset(dat, select = -c(recidivate, id, race, charge_name)))
# Determine number of in training and test set. Split accordingly.
n.total <- length(y)
prop.train <- 0.67
set.seed(123)
r <- sample(1:n.total, round(prop.train*n.total), replace = FALSE)
x.train <- x[r,]
x.test <- x[-r,]
y.train <- y[r]
y.test <- y[-r]
lasso.cv <- cv.glmnet(x = x.train, y = y.train,
family = "binomial", nfolds = 10, alpha = 1)
plot(lasso.cv)
opt.lambda <- lasso.cv$lambda.1se
opt.model <-  glmnet(x = x.train, y = y.train, alpha = 1, lambda = opt.lambda, family = "binomial")
test.predprob <- predict(opt.model, newx = x.test, type = "response")
yhat.test <- as.numeric(test.predprob > 0.5)
# accuracy
accLASSO.test <- mean(y.test == yhat.test)
# precision
precLASSO.test <- mean(y.test[yhat.test == 1] == 1)
# recall
recallLASSO.test <- mean(yhat.test[y.test == 1] == 1)
rr.cv <- cv.glmnet(x = x.train, y = y.train,
family = "binomial", nfolds = 10, alpha = 0)
plot(rr.cv)
opt.lambda.rr <- rr.cv$lambda.1se
opt.model.rr <-  glmnet(x = x.train, y = y.train, alpha = 0, lambda = opt.lambda.rr, family = "binomial")
test.predprob.rr <- predict(opt.model.rr, newx = x.test, type = "response")
yhat.test.rr <- as.numeric(test.predprob.rr > 0.5)
# accuracy
accRR.test <- mean(y.test == yhat.test.rr)
# precision
precRR.test <- mean(y.test[yhat.test.rr == 1] == 1)
# recall
recallRR.test <- mean(yhat.test.rr[y.test == 1] == 1)
rr <- c(accRR.test, precRR.test, recallRR.test)
lasso <- c(accLASSO.test, precLASSO.test, recallLASSO.test)
data.frame("rr_outofsample" = rr, "lasso_outofsample" = lasso, row.names = c("accuracy", "precision", "recall"))
# OPTION 3: Exclude nominal and charge_name (id,race, & charge_name)
x <- as.matrix(subset(dat, select = -c(recidivate, id, charge_name)))
# Determine number of in training and test set. Split accordingly.
n.total <- length(y)
prop.train <- 0.67
set.seed(123)
r <- sample(1:n.total, round(prop.train*n.total), replace = FALSE)
x.train <- x[r,]
x.test <- x[-r,]
y.train <- y[r]
y.test <- y[-r]
## LASSO
**Step 1: ** Find optimal lambda value using cross validation.
```{r}
lasso.cv <- cv.glmnet(x = x.train, y = y.train,
family = "binomial", nfolds = 10, alpha = 1)
plot(lasso.cv)
```
**Step 2: ** Fit model with optimal lambda.
```{r}
opt.lambda <- lasso.cv$lambda.1se
opt.model <-  glmnet(x = x.train, y = y.train, alpha = 1, lambda = opt.lambda, family = "binomial")
```
**Step 3: ** Predict onto test data.
```{r}
test.predprob <- predict(opt.model, newx = x.test, type = "response")
yhat.test <- as.numeric(test.predprob > 0.5)
# accuracy
accLASSO.test <- mean(y.test == yhat.test)
# precision
precLASSO.test <- mean(y.test[yhat.test == 1] == 1)
# recall
recallLASSO.test <- mean(yhat.test[y.test == 1] == 1)
# recall
recallLASSO.test <- mean(yhat.test[y.test == 1] == 1)
## RIDGE REGRESSION
**Step 1: ** Find optimal lambda value using cross validation.
```{r}
rr.cv <- cv.glmnet(x = x.train, y = y.train,
family = "binomial", nfolds = 10, alpha = 0)
plot(rr.cv)
```
**Step 2: ** Fit model with optimal lambda.
```{r}
opt.lambda.rr <- rr.cv$lambda.1se
opt.model.rr <-  glmnet(x = x.train, y = y.train, alpha = 0, lambda = opt.lambda.rr, family = "binomial")
```
**Step 3: ** Predict onto test data and find error metrics.
```{r}
test.predprob.rr <- predict(opt.model.rr, newx = x.test, type = "response")
yhat.test.rr <- as.numeric(test.predprob.rr > 0.5)
# accuracy
accRR.test <- mean(y.test == yhat.test.rr)
# precision
precRR.test <- mean(y.test[yhat.test.rr == 1] == 1)
# recall
recallRR.test <- mean(yhat.test.rr[y.test == 1] == 1)
# recall
recallRR.test <- mean(yhat.test.rr[y.test == 1] == 1)
## Results
```{r}
rr <- c(accRR.test, precRR.test, recallRR.test)
lasso <- c(accLASSO.test, precLASSO.test, recallLASSO.test)
data.frame("rr_outofsample" = rr, "lasso_outofsample" = lasso, row.names = c("accuracy", "precision", "recall"))
rm(list = ls())
library(tidyr)
library(glmnet)
dat <- read.csv("recidivism_data_sample.csv")
# Split into predictor vector (y) and feature matrix (x)
y <- dat$recidivate
# =====================
# CREATING THE X MATRIX
# =====================
# OPTION 1: Include all variables
# x <- subset(dat, select = -c(recidivate))
#x <- model.matrix(~.-1, x[,]) # Convert all categorical variable to dummies.
# OPTION 2: Exclude nominal data (id & race), include charge_name as dummies.
# x <- subset(dat, select = -c(recidivate, id, race))
# x <- model.matrix(~.-1, x[,]) # Convert all categorical variable to dummies.
# OPTION 3: Exclude nominal and charge_name (id,race, & charge_name)
x <- as.matrix(subset(dat, select = -c(recidivate, id, charge_name)))
# Determine number of in training and test set. Split accordingly.
n.total <- length(y)
prop.train <- 0.67
set.seed(123)
r <- sample(1:n.total, round(prop.train*n.total), replace = FALSE)
x.train <- x[r,]
x.test <- x[-r,]
y.train <- y[r]
y.test <- y[-r]
lasso.cv <- cv.glmnet(x = x.train, y = y.train,
family = "binomial", nfolds = 10, alpha = 1)
plot(lasso.cv)
View(x)
lasso.cv <- cv.glmnet(x = x.train, y = y.train,
family = "binomial", nfolds = 10, alpha = 1)
plot(lasso.cv)
opt.lambda <- lasso.cv$lambda.1se
opt.model <-  glmnet(x = x.train, y = y.train, alpha = 1, lambda = opt.lambda, family = "binomial")
test.predprob <- predict(opt.model, newx = x.test, type = "response")
yhat.test <- as.numeric(test.predprob > 0.5)
# accuracy
accLASSO.test <- mean(y.test == yhat.test)
# precision
precLASSO.test <- mean(y.test[yhat.test == 1] == 1)
# recall
recallLASSO.test <- mean(yhat.test[y.test == 1] == 1)
rr.cv <- cv.glmnet(x = x.train, y = y.train,
family = "binomial", nfolds = 10, alpha = 0)
plot(rr.cv)
opt.lambda.rr <- rr.cv$lambda.1se
opt.model.rr <-  glmnet(x = x.train, y = y.train, alpha = 0, lambda = opt.lambda.rr, family = "binomial")
test.predprob.rr <- predict(opt.model.rr, newx = x.test, type = "response")
yhat.test.rr <- as.numeric(test.predprob.rr > 0.5)
# accuracy
accRR.test <- mean(y.test == yhat.test.rr)
# precision
precRR.test <- mean(y.test[yhat.test.rr == 1] == 1)
# recall
recallRR.test <- mean(yhat.test.rr[y.test == 1] == 1)
rr <- c(accRR.test, precRR.test, recallRR.test)
lasso <- c(accLASSO.test, precLASSO.test, recallLASSO.test)
data.frame("rr_outofsample" = rr, "lasso_outofsample" = lasso, row.names = c("accuracy", "precision", "recall"))
rm(list = ls())
library(tidyr)
library(glmnet)
dat <- read.csv("recidivism_data_sample.csv")
# Split into predictor vector (y) and feature matrix (x)
y <- dat$recidivate
# =====================
# CREATING THE X MATRIX
# =====================
# OPTION 1: Include all variables
# x <- subset(dat, select = -c(recidivate))
#x <- model.matrix(~.-1, x[,]) # Convert all categorical variable to dummies.
# OPTION 2: Exclude nominal data (id & race), include charge_name as dummies.
# x <- subset(dat, select = -c(recidivate, id, race))
# x <- model.matrix(~.-1, x[,]) # Convert all categorical variable to dummies.
# OPTION 3: Exclude nominal and charge_name (id,race, & charge_name)
x <- as.matrix(subset(dat, select = -c(recidivate, id, race, charge_name)))
# Determine number of in training and test set. Split accordingly.
n.total <- length(y)
prop.train <- 0.67
set.seed(123)
r <- sample(1:n.total, round(prop.train*n.total), replace = FALSE)
x.train <- x[r,]
x.test <- x[-r,]
y.train <- y[r]
y.test <- y[-r]
lasso.cv <- cv.glmnet(x = x.train, y = y.train,
family = "binomial", nfolds = 10, alpha = 1)
plot(lasso.cv)
opt.lambda <- lasso.cv$lambda.1se
opt.model <-  glmnet(x = x.train, y = y.train, alpha = 1, lambda = opt.lambda, family = "binomial")
test.predprob <- predict(opt.model, newx = x.test, type = "response")
yhat.test <- as.numeric(test.predprob > 0.5)
# accuracy
accLASSO.test <- mean(y.test == yhat.test)
# precision
precLASSO.test <- mean(y.test[yhat.test == 1] == 1)
# recall
recallLASSO.test <- mean(yhat.test[y.test == 1] == 1)
rr.cv <- cv.glmnet(x = x.train, y = y.train,
family = "binomial", nfolds = 10, alpha = 0)
plot(rr.cv)
opt.lambda.rr <- rr.cv$lambda.1se
opt.model.rr <-  glmnet(x = x.train, y = y.train, alpha = 0, lambda = opt.lambda.rr, family = "binomial")
test.predprob.rr <- predict(opt.model.rr, newx = x.test, type = "response")
yhat.test.rr <- as.numeric(test.predprob.rr > 0.5)
# accuracy
accRR.test <- mean(y.test == yhat.test.rr)
# precision
precRR.test <- mean(y.test[yhat.test.rr == 1] == 1)
# recall
recallRR.test <- mean(yhat.test.rr[y.test == 1] == 1)
rr <- c(accRR.test, precRR.test, recallRR.test)
lasso <- c(accLASSO.test, precLASSO.test, recallLASSO.test)
data.frame("rr_outofsample" = rr, "lasso_outofsample" = lasso, row.names = c("accuracy", "precision", "recall"))
# OPTION 3: Exclude nominal and charge_name (id,race, & charge_name)
x <- as.matrix(subset(dat, select = -c(recidivate, id, charge_name)))
# Determine number of in training and test set. Split accordingly.
n.total <- length(y)
prop.train <- 0.67
set.seed(123)
r <- sample(1:n.total, round(prop.train*n.total), replace = FALSE)
x.train <- x[r,]
x.test <- x[-r,]
y.train <- y[r]
y.test <- y[-r]
## LASSO
**Step 1: ** Find optimal lambda value using cross validation.
```{r}
lasso.cv <- cv.glmnet(x = x.train, y = y.train,
family = "binomial", nfolds = 10, alpha = 1)
plot(lasso.cv)
```
**Step 2: ** Fit model with optimal lambda.
```{r}
opt.lambda <- lasso.cv$lambda.1se
opt.model <-  glmnet(x = x.train, y = y.train, alpha = 1, lambda = opt.lambda, family = "binomial")
```
**Step 3: ** Predict onto test data.
```{r}
test.predprob <- predict(opt.model, newx = x.test, type = "response")
yhat.test <- as.numeric(test.predprob > 0.5)
# accuracy
accLASSO.test <- mean(y.test == yhat.test)
# precision
precLASSO.test <- mean(y.test[yhat.test == 1] == 1)
# recall
recallLASSO.test <- mean(yhat.test[y.test == 1] == 1)
# recall
recallLASSO.test <- mean(yhat.test[y.test == 1] == 1)
## RIDGE REGRESSION
**Step 1: ** Find optimal lambda value using cross validation.
```{r}
rr.cv <- cv.glmnet(x = x.train, y = y.train,
family = "binomial", nfolds = 10, alpha = 0)
plot(rr.cv)
```
rr.cv <- cv.glmnet(x = x.train, y = y.train,
family = "binomial", nfolds = 10, alpha = 0)
plot(rr.cv)
opt.lambda.rr <- rr.cv$lambda.1se
opt.model.rr <-  glmnet(x = x.train, y = y.train, alpha = 0, lambda = opt.lambda.rr, family = "binomial")
test.predprob.rr <- predict(opt.model.rr, newx = x.test, type = "response")
yhat.test.rr <- as.numeric(test.predprob.rr > 0.5)
# accuracy
accRR.test <- mean(y.test == yhat.test.rr)
# precision
precRR.test <- mean(y.test[yhat.test.rr == 1] == 1)
# recall
recallRR.test <- mean(yhat.test.rr[y.test == 1] == 1)
# Split into predictor vector (y) and feature matrix (x)
y <- dat$recidivate
# =====================
# CREATING THE X MATRIX
# =====================
# OPTION 1: Include all variables
# x <- subset(dat, select = -c(recidivate))
#x <- model.matrix(~.-1, x[,]) # Convert all categorical variable to dummies.
# OPTION 2: Exclude nominal data (id & race), include charge_name as dummies.
# x <- subset(dat, select = -c(recidivate, id, race))
# x <- model.matrix(~.-1, x[,]) # Convert all categorical variable to dummies.
# OPTION 3: Exclude nominal and charge_name (id,race, & charge_name)
x <- as.matrix(subset(dat, select = -c(recidivate, id, charge_name)))
# Determine number of in training and test set. Split accordingly.
n.total <- length(y)
prop.train <- 0.67
set.seed(123)
r <- sample(1:n.total, round(prop.train*n.total), replace = FALSE)
x.train <- x[r,]
x.test <- x[-r,]
y.train <- y[r]
y.test <- y[-r]
lasso.cv <- cv.glmnet(x = x.train, y = y.train,
family = "binomial", nfolds = 10, alpha = 1)
plot(lasso.cv)
opt.lambda <- lasso.cv$lambda.1se
opt.model <-  glmnet(x = x.train, y = y.train, alpha = 1, lambda = opt.lambda, family = "binomial")
test.predprob <- predict(opt.model, newx = x.test, type = "response")
yhat.test <- as.numeric(test.predprob > 0.5)
# accuracy
accLASSO.test <- mean(y.test == yhat.test)
# precision
precLASSO.test <- mean(y.test[yhat.test == 1] == 1)
# recall
recallLASSO.test <- mean(yhat.test[y.test == 1] == 1)
rr.cv <- cv.glmnet(x = x.train, y = y.train,
family = "binomial", nfolds = 10, alpha = 0)
plot(rr.cv)
opt.lambda.rr <- rr.cv$lambda.1se
opt.model.rr <-  glmnet(x = x.train, y = y.train, alpha = 0, lambda = opt.lambda.rr, family = "binomial")
test.predprob.rr <- predict(opt.model.rr, newx = x.test, type = "response")
yhat.test.rr <- as.numeric(test.predprob.rr > 0.5)
# accuracy
accRR.test <- mean(y.test == yhat.test.rr)
# precision
precRR.test <- mean(y.test[yhat.test.rr == 1] == 1)
# recall
recallRR.test <- mean(yhat.test.rr[y.test == 1] == 1)
rr <- c(accRR.test, precRR.test, recallRR.test)
lasso <- c(accLASSO.test, precLASSO.test, recallLASSO.test)
data.frame("rr_outofsample" = rr, "lasso_outofsample" = lasso, row.names = c("accuracy", "precision", "recall"))
rr <- c(accRR.test, precRR.test, recallRR.test)
lasso <- c(accLASSO.test, precLASSO.test, recallLASSO.test)
data.frame("rr_outofsample" = rr, "lasso_outofsample" = lasso, row.names = c("accuracy", "precision", "recall"))
rr <- c(accRR.test, precRR.test, recallRR.test)
lasso <- c(accLASSO.test, precLASSO.test, recallLASSO.test)
data.frame("rr_outofsample" = rr, "lasso_outofsample" = lasso, row.names = c("accuracy", "precision", "recall"))
rr <- c(accRR.test, precRR.test, recallRR.test)
lasso <- c(accLASSO.test, precLASSO.test, recallLASSO.test)
data.frame("rr_outofsample" = rr, "lasso_outofsample" = lasso, row.names = c("accuracy", "precision", "recall"))
rr <- c(accRR.test, precRR.test, recallRR.test)
lasso <- c(accLASSO.test, precLASSO.test, recallLASSO.test)
data.frame("rr_outofsample" = rr, "lasso_outofsample" = lasso, row.names = c("accuracy", "precision", "recall"))
rm(list = ls())
library(tidyr)
library(glmnet)
dat <- read.csv("recidivism_data_sample.csv")
# Split into predictor vector (y) and feature matrix (x)
y <- dat$recidivate
# =====================
# CREATING THE X MATRIX
# =====================
# OPTION 1: Include all variables
# x <- subset(dat, select = -c(recidivate))
#x <- model.matrix(~.-1, x[,]) # Convert all categorical variable to dummies.
# OPTION 2: Exclude nominal data (id & race), include charge_name as dummies.
# x <- subset(dat, select = -c(recidivate, id, race))
# x <- model.matrix(~.-1, x[,]) # Convert all categorical variable to dummies.
# OPTION 3: Exclude nominal and charge_name (id,race, & charge_name)
x <- as.matrix(subset(dat, select = -c(recidivate, id, charge_name)))
# Determine number of in training and test set. Split accordingly.
n.total <- length(y)
prop.train <- 0.67
set.seed(123)
r <- sample(1:n.total, round(prop.train*n.total), replace = FALSE)
x.train <- x[r,]
x.test <- x[-r,]
y.train <- y[r]
y.test <- y[-r]
lasso.cv <- cv.glmnet(x = x.train, y = y.train,
family = "binomial", nfolds = 10, alpha = 1)
plot(lasso.cv)
rm(list = ls())
library(tidyr)
library(glmnet)
dat <- read.csv("recidivism_data_sample.csv")
# Split into predictor vector (y) and feature matrix (x)
y <- dat$recidivate
# =====================
# CREATING THE X MATRIX
# =====================
# OPTION 1: Include all variables
# x <- subset(dat, select = -c(recidivate))
#x <- model.matrix(~.-1, x[,]) # Convert all categorical variable to dummies.
# OPTION 2: Exclude nominal data (id & race), include charge_name as dummies.
# x <- subset(dat, select = -c(recidivate, id, race))
# x <- model.matrix(~.-1, x[,]) # Convert all categorical variable to dummies.
# OPTION 3: Exclude nominal and charge_name (id,race, & charge_name)
x <- as.matrix(subset(dat, select = -c(recidivate, id, race,charge_name)))
# Determine number of in training and test set. Split accordingly.
n.total <- length(y)
prop.train <- 0.67
set.seed(123)
r <- sample(1:n.total, round(prop.train*n.total), replace = FALSE)
x.train <- x[r,]
x.test <- x[-r,]
y.train <- y[r]
y.test <- y[-r]
lasso.cv <- cv.glmnet(x = x.train, y = y.train,
family = "binomial", nfolds = 10, alpha = 1)
plot(lasso.cv)
opt.lambda <- lasso.cv$lambda.1se
opt.model <-  glmnet(x = x.train, y = y.train, alpha = 1, lambda = opt.lambda, family = "binomial")
test.predprob <- predict(opt.model, newx = x.test, type = "response")
yhat.test <- as.numeric(test.predprob > 0.5)
# accuracy
accLASSO.test <- mean(y.test == yhat.test)
# precision
precLASSO.test <- mean(y.test[yhat.test == 1] == 1)
# recall
recallLASSO.test <- mean(yhat.test[y.test == 1] == 1)
summary(opt.model)
opt.model
test.predprob <- predict(opt.model, newx = x.test, type = "response")
yhat.test <- as.numeric(test.predprob > 0.5)
# accuracy
accLASSO.test <- mean(y.test == yhat.test)
# precision
precLASSO.test <- mean(y.test[yhat.test == 1] == 1)
# recall
recallLASSO.test <- mean(yhat.test[y.test == 1] == 1)
rr.cv <- cv.glmnet(x = x.train, y = y.train,
family = "binomial", nfolds = 10, alpha = 0)
plot(rr.cv)
opt.lambda.rr <- rr.cv$lambda.1se
opt.model.rr <-  glmnet(x = x.train, y = y.train, alpha = 0, lambda = opt.lambda.rr, family = "binomial")
test.predprob.rr <- predict(opt.model.rr, newx = x.test, type = "response")
yhat.test.rr <- as.numeric(test.predprob.rr > 0.5)
# accuracy
accRR.test <- mean(y.test == yhat.test.rr)
# precision
precRR.test <- mean(y.test[yhat.test.rr == 1] == 1)
# recall
recallRR.test <- mean(yhat.test.rr[y.test == 1] == 1)
rr <- c(accRR.test, precRR.test, recallRR.test)
lasso <- c(accLASSO.test, precLASSO.test, recallLASSO.test)
data.frame("rr_outofsample" = rr, "lasso_outofsample" = lasso, row.names = c("accuracy", "precision", "recall"))
