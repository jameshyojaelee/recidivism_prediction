# x <- subset(dat, select = -c(recidivate))
#x <- model.matrix(~.-1, x[,]) # Convert all categorical variable to dummies.
# OPTION 2: Exclude nominal data (id & race), include charge_name as dummies.
# x <- subset(dat, select = -c(recidivate, id, race))
# x <- model.matrix(~.-1, x[,]) # Convert all categorical variable to dummies.
# OPTION 3: Exclude nominal and charge_name (id,race, & charge_name)
x <- as.matrix(subset(dat, select = -c(recidivate, id, race, charge_name)))
# Determine number of in training and test set. Split accordingly.
n.total <- length(y)
prop.train <- 0.67
set.seed(123)
r <- sample(1:n.total, round(prop.train*n.total), replace = FALSE)
x.train <- x[r,]
x.test <- x[-r,]
y.train <- y[r]
y.test <- y[-r]
lasso.cv <- cv.glmnet(x = x.train, y = y.train,
family = "binomial", nfolds = 10, alpha = 1)
plot(lasso.cv)
opt.lambda <- lasso.cv$lambda.1se
opt.model <-  glmnet(x = x.train, y = y.train, alpha = 1, lambda = opt.lambda, family = "binomial")
test.predprob <- predict(opt.model, newx = x.test, type = "response")
yhat.test <- as.numeric(test.predprob > 0.5)
# accuracy
accLASSO.test <- mean(y.test == yhat.test)
# precision
precLASSO.test <- mean(y.test[yhat.test == 1] == 1)
# recall
recallLASSO.test <- mean(yhat.test[y.test == 1] == 1)
rr.cv <- cv.glmnet(x = x.train, y = y.train,
family = "binomial", nfolds = 10, alpha = 0)
plot(rr.cv)
opt.lambda.rr <- rr.cv$lambda.1se
opt.model.rr <-  glmnet(x = x.train, y = y.train, alpha = 0, lambda = opt.lambda.rr, family = "binomial")
test.predprob.rr <- predict(opt.model.rr, newx = x.test, type = "response")
yhat.test.rr <- as.numeric(test.predprob.rr > 0.5)
# accuracy
accRR.test <- mean(y.test == yhat.test.rr)
# precision
precRR.test <- mean(y.test[yhat.test.rr == 1] == 1)
# recall
recallRR.test <- mean(yhat.test.rr[y.test == 1] == 1)
rr <- c(accRR.test, precRR.test, recallRR.test)
lasso <- c(accLASSO.test, precLASSO.test, recallLASSO.test)
data.frame("rr_outofsample" = rr, "lasso_outofsample" = lasso, row.names = c("accuracy", "precision", "recall"))
# OPTION 3: Exclude nominal and charge_name (id,race, & charge_name)
x <- as.matrix(subset(dat, select = -c(recidivate, id, charge_name)))
# Determine number of in training and test set. Split accordingly.
n.total <- length(y)
prop.train <- 0.67
set.seed(123)
r <- sample(1:n.total, round(prop.train*n.total), replace = FALSE)
x.train <- x[r,]
x.test <- x[-r,]
y.train <- y[r]
y.test <- y[-r]
## LASSO
**Step 1: ** Find optimal lambda value using cross validation.
```{r}
lasso.cv <- cv.glmnet(x = x.train, y = y.train,
family = "binomial", nfolds = 10, alpha = 1)
plot(lasso.cv)
```
**Step 2: ** Fit model with optimal lambda.
```{r}
opt.lambda <- lasso.cv$lambda.1se
opt.model <-  glmnet(x = x.train, y = y.train, alpha = 1, lambda = opt.lambda, family = "binomial")
```
**Step 3: ** Predict onto test data.
```{r}
test.predprob <- predict(opt.model, newx = x.test, type = "response")
yhat.test <- as.numeric(test.predprob > 0.5)
# accuracy
accLASSO.test <- mean(y.test == yhat.test)
# precision
precLASSO.test <- mean(y.test[yhat.test == 1] == 1)
# recall
recallLASSO.test <- mean(yhat.test[y.test == 1] == 1)
# recall
recallLASSO.test <- mean(yhat.test[y.test == 1] == 1)
## RIDGE REGRESSION
**Step 1: ** Find optimal lambda value using cross validation.
```{r}
rr.cv <- cv.glmnet(x = x.train, y = y.train,
family = "binomial", nfolds = 10, alpha = 0)
plot(rr.cv)
```
rr.cv <- cv.glmnet(x = x.train, y = y.train,
family = "binomial", nfolds = 10, alpha = 0)
plot(rr.cv)
opt.lambda.rr <- rr.cv$lambda.1se
opt.model.rr <-  glmnet(x = x.train, y = y.train, alpha = 0, lambda = opt.lambda.rr, family = "binomial")
test.predprob.rr <- predict(opt.model.rr, newx = x.test, type = "response")
yhat.test.rr <- as.numeric(test.predprob.rr > 0.5)
# accuracy
accRR.test <- mean(y.test == yhat.test.rr)
# precision
precRR.test <- mean(y.test[yhat.test.rr == 1] == 1)
# recall
recallRR.test <- mean(yhat.test.rr[y.test == 1] == 1)
# Split into predictor vector (y) and feature matrix (x)
y <- dat$recidivate
# =====================
# CREATING THE X MATRIX
# =====================
# OPTION 1: Include all variables
# x <- subset(dat, select = -c(recidivate))
#x <- model.matrix(~.-1, x[,]) # Convert all categorical variable to dummies.
# OPTION 2: Exclude nominal data (id & race), include charge_name as dummies.
# x <- subset(dat, select = -c(recidivate, id, race))
# x <- model.matrix(~.-1, x[,]) # Convert all categorical variable to dummies.
# OPTION 3: Exclude nominal and charge_name (id,race, & charge_name)
x <- as.matrix(subset(dat, select = -c(recidivate, id, charge_name)))
# Determine number of in training and test set. Split accordingly.
n.total <- length(y)
prop.train <- 0.67
set.seed(123)
r <- sample(1:n.total, round(prop.train*n.total), replace = FALSE)
x.train <- x[r,]
x.test <- x[-r,]
y.train <- y[r]
y.test <- y[-r]
lasso.cv <- cv.glmnet(x = x.train, y = y.train,
family = "binomial", nfolds = 10, alpha = 1)
plot(lasso.cv)
opt.lambda <- lasso.cv$lambda.1se
opt.model <-  glmnet(x = x.train, y = y.train, alpha = 1, lambda = opt.lambda, family = "binomial")
test.predprob <- predict(opt.model, newx = x.test, type = "response")
yhat.test <- as.numeric(test.predprob > 0.5)
# accuracy
accLASSO.test <- mean(y.test == yhat.test)
# precision
precLASSO.test <- mean(y.test[yhat.test == 1] == 1)
# recall
recallLASSO.test <- mean(yhat.test[y.test == 1] == 1)
rr.cv <- cv.glmnet(x = x.train, y = y.train,
family = "binomial", nfolds = 10, alpha = 0)
plot(rr.cv)
opt.lambda.rr <- rr.cv$lambda.1se
opt.model.rr <-  glmnet(x = x.train, y = y.train, alpha = 0, lambda = opt.lambda.rr, family = "binomial")
test.predprob.rr <- predict(opt.model.rr, newx = x.test, type = "response")
yhat.test.rr <- as.numeric(test.predprob.rr > 0.5)
# accuracy
accRR.test <- mean(y.test == yhat.test.rr)
# precision
precRR.test <- mean(y.test[yhat.test.rr == 1] == 1)
# recall
recallRR.test <- mean(yhat.test.rr[y.test == 1] == 1)
rr <- c(accRR.test, precRR.test, recallRR.test)
lasso <- c(accLASSO.test, precLASSO.test, recallLASSO.test)
data.frame("rr_outofsample" = rr, "lasso_outofsample" = lasso, row.names = c("accuracy", "precision", "recall"))
rr <- c(accRR.test, precRR.test, recallRR.test)
lasso <- c(accLASSO.test, precLASSO.test, recallLASSO.test)
data.frame("rr_outofsample" = rr, "lasso_outofsample" = lasso, row.names = c("accuracy", "precision", "recall"))
rr <- c(accRR.test, precRR.test, recallRR.test)
lasso <- c(accLASSO.test, precLASSO.test, recallLASSO.test)
data.frame("rr_outofsample" = rr, "lasso_outofsample" = lasso, row.names = c("accuracy", "precision", "recall"))
rr <- c(accRR.test, precRR.test, recallRR.test)
lasso <- c(accLASSO.test, precLASSO.test, recallLASSO.test)
data.frame("rr_outofsample" = rr, "lasso_outofsample" = lasso, row.names = c("accuracy", "precision", "recall"))
rr <- c(accRR.test, precRR.test, recallRR.test)
lasso <- c(accLASSO.test, precLASSO.test, recallLASSO.test)
data.frame("rr_outofsample" = rr, "lasso_outofsample" = lasso, row.names = c("accuracy", "precision", "recall"))
rm(list = ls())
library(tidyr)
library(glmnet)
dat <- read.csv("recidivism_data_sample.csv")
# Split into predictor vector (y) and feature matrix (x)
y <- dat$recidivate
# =====================
# CREATING THE X MATRIX
# =====================
# OPTION 1: Include all variables
# x <- subset(dat, select = -c(recidivate))
#x <- model.matrix(~.-1, x[,]) # Convert all categorical variable to dummies.
# OPTION 2: Exclude nominal data (id & race), include charge_name as dummies.
# x <- subset(dat, select = -c(recidivate, id, race))
# x <- model.matrix(~.-1, x[,]) # Convert all categorical variable to dummies.
# OPTION 3: Exclude nominal and charge_name (id,race, & charge_name)
x <- as.matrix(subset(dat, select = -c(recidivate, id, charge_name)))
# Determine number of in training and test set. Split accordingly.
n.total <- length(y)
prop.train <- 0.67
set.seed(123)
r <- sample(1:n.total, round(prop.train*n.total), replace = FALSE)
x.train <- x[r,]
x.test <- x[-r,]
y.train <- y[r]
y.test <- y[-r]
lasso.cv <- cv.glmnet(x = x.train, y = y.train,
family = "binomial", nfolds = 10, alpha = 1)
plot(lasso.cv)
rm(list = ls())
library(tidyr)
library(glmnet)
dat <- read.csv("recidivism_data_sample.csv")
# Split into predictor vector (y) and feature matrix (x)
y <- dat$recidivate
# =====================
# CREATING THE X MATRIX
# =====================
# OPTION 1: Include all variables
# x <- subset(dat, select = -c(recidivate))
#x <- model.matrix(~.-1, x[,]) # Convert all categorical variable to dummies.
# OPTION 2: Exclude nominal data (id & race), include charge_name as dummies.
# x <- subset(dat, select = -c(recidivate, id, race))
# x <- model.matrix(~.-1, x[,]) # Convert all categorical variable to dummies.
# OPTION 3: Exclude nominal and charge_name (id,race, & charge_name)
x <- as.matrix(subset(dat, select = -c(recidivate, id, race,charge_name)))
# Determine number of in training and test set. Split accordingly.
n.total <- length(y)
prop.train <- 0.67
set.seed(123)
r <- sample(1:n.total, round(prop.train*n.total), replace = FALSE)
x.train <- x[r,]
x.test <- x[-r,]
y.train <- y[r]
y.test <- y[-r]
lasso.cv <- cv.glmnet(x = x.train, y = y.train,
family = "binomial", nfolds = 10, alpha = 1)
plot(lasso.cv)
opt.lambda <- lasso.cv$lambda.1se
opt.model <-  glmnet(x = x.train, y = y.train, alpha = 1, lambda = opt.lambda, family = "binomial")
test.predprob <- predict(opt.model, newx = x.test, type = "response")
yhat.test <- as.numeric(test.predprob > 0.5)
# accuracy
accLASSO.test <- mean(y.test == yhat.test)
# precision
precLASSO.test <- mean(y.test[yhat.test == 1] == 1)
# recall
recallLASSO.test <- mean(yhat.test[y.test == 1] == 1)
summary(opt.model)
opt.model
test.predprob <- predict(opt.model, newx = x.test, type = "response")
yhat.test <- as.numeric(test.predprob > 0.5)
# accuracy
accLASSO.test <- mean(y.test == yhat.test)
# precision
precLASSO.test <- mean(y.test[yhat.test == 1] == 1)
# recall
recallLASSO.test <- mean(yhat.test[y.test == 1] == 1)
rr.cv <- cv.glmnet(x = x.train, y = y.train,
family = "binomial", nfolds = 10, alpha = 0)
plot(rr.cv)
opt.lambda.rr <- rr.cv$lambda.1se
opt.model.rr <-  glmnet(x = x.train, y = y.train, alpha = 0, lambda = opt.lambda.rr, family = "binomial")
test.predprob.rr <- predict(opt.model.rr, newx = x.test, type = "response")
yhat.test.rr <- as.numeric(test.predprob.rr > 0.5)
# accuracy
accRR.test <- mean(y.test == yhat.test.rr)
# precision
precRR.test <- mean(y.test[yhat.test.rr == 1] == 1)
# recall
recallRR.test <- mean(yhat.test.rr[y.test == 1] == 1)
rr <- c(accRR.test, precRR.test, recallRR.test)
lasso <- c(accLASSO.test, precLASSO.test, recallLASSO.test)
data.frame("rr_outofsample" = rr, "lasso_outofsample" = lasso, row.names = c("accuracy", "precision", "recall"))
# boosted tree
rm(list = ls())
library(gbm)
rd <- read.csv("recidivism_data_sample.csv")
head(rd)
#drop ID because it's meaningless
rd <- rd[,-1]
#convert race into factor variables
rd$race <- as.factor(rd$race)
# Data split
set.seed(1234)
n <- nrow(rd)
v <- sample(n,4000,replace = FALSE)
train.dat <- rd[v,]
test.dat <- rd[-v,]
# Boosted Trees CV
depths <- c(1,2,3,4,5,6,7,8)
bt.models <- list()
set.seed(1234)
for (i in 1:length(depths)){
bt.models[[i]] <- gbm(recidivate ~  .,
data = train.dat, shrinkage = 0.01,
distribution="bernoulli", n.trees = 2000,
interaction.depth = depths[i],
cv.folds = 10)
print(i)
}
save(bt.models, file = "bt_cv_models.Rdata")
load("bt_cv_models.Rdata")
n.depths <- length(bt.models)
depths <- rep(NA, n.depths)
min.cv.error <- rep(NA, n.depths)
best.n.trees <- rep(NA, n.depths)
for (i in 1:n.depths){
bt.curr <- bt.models[[i]]
depths[i] <- bt.curr$interaction.depth
min.cv.error[i] <- min(bt.curr$cv.error)
best.n.trees[i] <- which.min(bt.curr$cv.error)
rm(bt.curr)
}
min.cv.error
m <- which.min(min.cv.error)
final.ntrees <- best.n.trees[m]
final.ntrees
final.depth <- depths[m]
final.depth
# final model
test.dat.preds <- predict(bt.models[[m]], newdata = test.dat,
n.trees = final.ntrees)
#Test set MSE:
mean((test.dat.preds - test.dat$recidivate)^2)
test.dat.preds
# final model
test.dat.preds <- predict(bt.models[[m]], newdata = test.dat,
n.trees = final.ntrees, type="prob")
# final model
test.dat.preds <- predict(bt.models[[m]], newdata = test.dat,
n.trees = final.ntrees, type="response")
test.dat.preds
# final model
test.dat.preds <- predict(bt.models[[m]], newdata = test.dat,
n.trees = final.ntrees, type="response")
#Test set MSE:
mean((test.dat.preds - test.dat$recidivate)^2)
# final model
test.dat.preds <- predict(bt.models[[m]], newdata = test.dat,
n.trees = final.ntrees, type="response")
#Test set MSE:
mean((test.dat.preds - test.dat$recidivate)^2)
knitr::opts_chunk$set(echo = TRUE)
library(glmnet)
library(ISLR)
library(randomForest)
library(gbm)
library(ggplot2)
library(dplyr)
library(tidyr)
dat <- read.csv("recidivism_data_sample.csv")
dat$race <- as.factor(dat$race)
k <- 10
n <- nrow(dat)
folds <- c(rep(1:k,600))
set.seed(131)
folds <- sample(folds,length(folds))
table(folds)
dat$yhat <- NA
data.folds <- list()
for (i in 1:k){
data.folds[[i]] <- dat[folds == i,]
}
for (i in 1:k){
train.dat <- do.call("rbind",data.folds[-i])
cv.mod <- glm(recidivate ~ as.factor(race) + sex + age + juv_fel_count + juv_misd_count + priors_count + charge_degree + charge_name, family = binomial(link="logit"), data = train.dat)
data.folds[[i]]$yhat <- predict(cv.mod, newdata = data.folds[[i]], type = "response")
rm(train.dat,cv.mod)
}
CV.dat <- do.call("rbind",data.folds)
log_mse <- mean(as.numeric((CV.dat$recidivate - CV.dat$yhat)^2), na.rm = TRUE)
CV.dat$yhat2 <- rep(NA,length(CV.dat$yhat))
for(i in 1:length(CV.dat$yhat)){
if (CV.dat$yhat[i] >= 0.5){
CV.dat$yhat2[i] = 1
}
if (CV.dat$yhat[i] < 0.5){
CV.dat$yhat2[i] = 0
}
}
class_err <- mean(as.numeric((CV.dat$recidivate - CV.dat$yhat2)^2),na.rm=TRUE)
# Split into predictor vector (y) and feature matrix (x)
y <- dat$recidivate
# =====================
# CREATING THE X MATRIX
# =====================
x <- subset(dat, select = -c(recidivate))
x <- model.matrix(~.-1, x[,]) # Convert all categorical variable to dummies.
n.total <- length(y)
prop.train <- 0.67
set.seed(123)
r <- sample(1:n.total, round(prop.train*n.total), replace = FALSE)
x.train <- x[r,]
x
x.test <- x[-r,]
y.train <- y[r]
y.test <- y[-r]
x.train <- x[r,]
# =====================
# CREATING THE X MATRIX
# =====================
x <- subset(dat, select = -c(recidivate))
# Split into predictor vector (y) and feature matrix (x)
y <- dat$recidivate
# =====================
# CREATING THE X MATRIX
# =====================
x <- subset(dat, select = -c(recidivate))
#x <- model.matrix(~.-1, x[,]) # Convert all categorical variable to dummies.
n.total <- length(y)
prop.train <- 0.67
set.seed(123)
r <- sample(1:n.total, round(prop.train*n.total), replace = FALSE)
x.train <- x[r,]
x.test <- x[-r,]
y.train <- y[r]
y.test <- y[-r]
# Split into predictor vector (y) and feature matrix (x)
y <- dat$recidivate
# =====================
# CREATING THE X MATRIX
# =====================
x <- subset(dat, select = -c(recidivate))
#x <- model.matrix(~.-1, x[,]) # Convert all categorical variable to dummies.
n.total <- length(y)
prop.train <- 0.67
set.seed(123)
r <- sample(1:n.total, round(prop.train*n.total), replace = FALSE)
x.train <- x[r,]
x.test <- x[-r,]
y.train <- y[r]
y.test <- y[-r]
lasso.cv <- cv.glmnet(x = x.train, y = y.train,
family = "binomial", nfolds = 10, alpha = 1)
# Split into predictor vector (y) and feature matrix (x)
y <- dat$recidivate
# =====================
# CREATING THE X MATRIX
# =====================
x <- subset(dat, select = -c(recidivate))
#x <- model.matrix(~.-1, x[,]) # Convert all categorical variable to dummies.
n.total <- length(y)
prop.train <- 0.67
set.seed(123)
r <- sample(1:n.total, round(prop.train*n.total), replace = FALSE)
x.train <- x[r,]
x.test <- x[-r,]
y.train <- y[r]
y.test <- y[-r]
lasso.cv <- cv.glmnet(x = x.train, y = y.train,
family = "binomial", nfolds = 10, alpha = 1)
y.train
View(x.train)
y.train <- y[r,]
y.train <- y[r]
y.test <- y[-r]
lasso.cv <- cv.glmnet(x = x.train, y = y.train,
family = "binomial", nfolds = 10, alpha = 1)
# Split into predictor vector (y) and feature matrix (x)
y <- dat$recidivate
# =====================
# CREATING THE X MATRIX
# =====================
x <- subset(dat, select = -c(recidivate))
#x <- model.matrix(~.-1, x[,]) # Convert all categorical variable to dummies.
n.total <- length(y)
prop.train <- 0.67
set.seed(123)
r <- sample(1:n.total, round(prop.train*n.total), replace = FALSE)
x.train <- x[r,]
x.test <- x[-r,]
y.train <- y[r]
y.test <- y[-r]
```{r}
load("CreditClaim.RData")
x <- credit_claim$x
y <- credit_claim$y
n.total <- length(y)
prop.train <- 0.7
set.seed(54321)
r <- sample(1:n.total,round(prop.train*n.total), replace = FALSE)
x.train <- x[r,]
x.test <- x[-r,]
y.train <- y[r]
y.test <- y[-r]
x <- model.matrix(~.-1, x[,]) # Convert all categorical variable to dummies.
# Split into predictor vector (y) and feature matrix (x)
y <- dat$recidivate
# =====================
# CREATING THE X MATRIX
# =====================
x <- subset(dat, select = -c(recidivate))
x <- model.matrix(~.-1, x[,]) # Convert all categorical variable to dummies.
n.total <- length(y)
prop.train <- 0.67
set.seed(123)
r <- sample(1:n.total, round(prop.train*n.total), replace = FALSE)
x.train <- x[r,]
View(x)
# Split into predictor vector (y) and feature matrix (x)
y <- dat$recidivate
# =====================
# CREATING THE X MATRIX
# =====================
x <- subset(dat, select = -c(recidivate))
# x <- model.matrix(~.-1, x[,]) # Convert all categorical variable to dummies.
n.total <- length(y)
prop.train <- 0.67
set.seed(123)
r <- sample(1:n.total, round(prop.train*n.total), replace = FALSE)
x.train <- x[r,]
x.test <- x[-r,]
y.train <- y[r]
y.test <- y[-r]
# Split into predictor vector (y) and feature matrix (x)
y <- dat$recidivate
# =====================
# CREATING THE X MATRIX
# =====================
x <- subset(dat, select = -c(recidivate))
x <- model.matrix(~.-1, x[,]) # Convert all categorical variable to dummies.
n.total <- length(y)
prop.train <- 0.67
set.seed(123)
r <- sample(1:n.total, round(prop.train*n.total), replace = FALSE)
x.train <- x[r,]
rr.cv <- cv.glmnet(x = x.train, y = y.train,
family = "binomial", nfolds = 10, alpha = 0)
