---
title: 'Poli 175: Challenge 1'
author: "Emanuela Beale, Maya Lu and James Lee"
date: "2/11/2021"
output: html_document
---

```{r setup, include=FALSE}
library(glmnet)
library(ISLR)
library(randomForest)
library(gbm)
library(ggplot2)
library(dplyr)
library(tidyr)
```
### Load data
```{r}
dat <- read.csv("/Users/emanuelabeale/Downloads/recidivism_data_sample.csv")
dat = dat[, -1]
```

```{r}
for (i in 1:nrow(dat)){
  if (dat$race[i] == 1){
    dat$race[i] <- "White"
  }
  else if (dat$race[i] == 2){
    dat$race[i] <- "Black"
  }
  else if (dat$race[i] == 3){
    dat$race[i] <- "Hispanic"
  }
  else if (dat$race[i] == 4){
    dat$race[i] <- "Asian"
  }
  else if (dat$race[i] == 5){
    dat$race[i] <- "Native American"
  }
  else if (dat$race[i] == 6){
    dat$race[i] <- "Other"
  }
}
```

Because the dataset contains catagorical variables in the form of numbers, we should clarify that they should be treated as factors:
```{r}
dat$race <- as.factor(dat$race)
dat$sex <- as.factor(dat$sex)
dat$charge_name <- as.factor(dat$charge_name)
```

## MODEL 1: Logistic Regression
**Step 1: ** Choose number of folds and create partitions:
```{r}
k <- 10
n <- nrow(dat)
folds <- c(rep(1:k,600))
set.seed(131)
folds <- sample(folds,length(folds))
table(folds)
```

**Step 2: ** Implement partitions:
```{r}
dat$yhat <- NA
data.folds <- list()
for (i in 1:k){
  data.folds[[i]] <- dat[folds == i,]
}
```

**Step 3: ** Apply cross validation on logistic regression and make predictions:
```{r}
for (i in 1:k){
  train.dat <- do.call("rbind",data.folds[-i])
  cv.mod <- glm(recidivate ~ as.factor(race) + sex + age + juv_fel_count + juv_misd_count + priors_count + charge_degree + charge_name, family = binomial(link="logit"), data = train.dat)
  data.folds[[i]]$yhat <- predict(cv.mod, newdata = data.folds[[i]], type = "response")
  rm(train.dat,cv.mod)
}
CV.dat <- do.call("rbind",data.folds)
log.yhat <- CV.dat$yhat
```

## MODEL 2: Lasso Regression
### Setup   
Separate data into training and test set:
```{r}
# Split into predictor vector (y) and feature matrix (x)
y <- dat$recidivate
# =====================
# CREATING THE X MATRIX
# =====================
x <- subset(dat, select = -c(recidivate, yhat)) # remove yhat which was generated in log model
x <- model.matrix(~.-1, x[,]) # Convert all categorical variable to dummies.
n.total <- length(y)
prop.train <- 0.67 
set.seed(123)
r <- sample(1:n.total, round(prop.train*n.total), replace = FALSE) 
x.train <- x[r,]
x.test <- x[-r,]
y.train <- y[r]
y.test <- y[-r]
```

**Step 1: ** Find optimal lambda value using cross validation:
```{r}
lasso.cv <- cv.glmnet(x = x.train, y = y.train,
                             family = "binomial", nfolds = 10, alpha = 1)
plot(lasso.cv)
```

**Step 2: ** Fit model with optimal lambda:
```{r}
opt.lambda.lasso <- lasso.cv$lambda.1se
opt.model.lasso <-  glmnet(x = x.train, y = y.train, alpha = 1, lambda = opt.lambda.lasso, family = "binomial") 
```

**Step 3: ** Predict onto test data:
```{r}
lasso.yhat <- predict(opt.model.lasso, newx = x.test, type = "response")[,1]
```


## MODEL 3: Ridge Regression
**Step 1: ** Find optimal lambda value using cross validation: 
```{r}
rr.cv <- cv.glmnet(x = x.train, y = y.train,
                             family = "binomial", nfolds = 10, alpha = 0)
plot(rr.cv)
```

**Step 2: ** Fit model with optimal lambda:
```{r}
opt.lambda.rr <- rr.cv$lambda.1se
opt.model.rr <-  glmnet(x = x.train, y = y.train, alpha = 0, lambda = opt.lambda.rr, family = "binomial") 
```

**Step 3: ** Predict onto test data:
```{r}
rr.yhat <- predict(opt.model.rr, newx = x.test, type = "response")[,1]
```


## MODEL 4: Boosted Tree
### Setup:

**Step 1: ** Data split:
```{r}
dat <- dat[,1:9]
set.seed(1234)
n <- nrow(dat)
v <- sample(n,4000,replace = FALSE)
train.dat <- dat[v,]
test.dat <- dat[-v,]
```

**Step 2: ** Apply cross validation on boosted trees:
```{r}
depths <- c(1,2,3,4,5,6,7,8)
bt.models <- list()
set.seed(1234)
for (i in 1:length(depths)){
  bt.models[[i]] <- gbm(recidivate ~  ., 
                        data = train.dat, shrinkage = 0.01,
                        distribution="bernoulli", n.trees = 2000, 
                        interaction.depth = depths[i], 
                        cv.folds = 10)
  print(i)
}
save(bt.models, file = "bt_cv_models.Rdata")
load("bt_cv_models.Rdata")
n.depths <- length(bt.models)
depths <- rep(NA, n.depths)
min.cv.error <- rep(NA, n.depths)
best.n.trees <- rep(NA, n.depths)
for (i in 1:n.depths){
  bt.curr <- bt.models[[i]]
  depths[i] <- bt.curr$interaction.depth
  min.cv.error[i] <- min(bt.curr$cv.error)
  best.n.trees[i] <- which.min(bt.curr$cv.error)
  rm(bt.curr)
}
m <- which.min(min.cv.error)
final.ntrees <- best.n.trees[m]
final.depth <- depths[m]
```

**Step 3: ** Predict onto test data:
```{r}
bt.yhat <- predict(bt.models[[m]], newdata = test.dat, 
                          n.trees = final.ntrees, type="response")
```


## Model 5: Random Forest
**Step 1: ** Convert recidivate variable into factor:
```{r}
train.dat.class <- train.dat
train.dat.class$recidivate <- as.factor(train.dat.class$recidivate)
test.dat.class <- test.dat
test.dat.class$recidivate <- as.factor(test.dat.class$recidivate)
```

**Step 2: ** Apply cross validation to random forest:
```{r}
n.vars <- c(1,2,3,4,5,6,7,8)
rf.models <- list()
# use CV to choose m (number of variables to consider at each split)
set.seed(3956)
for (i in 1:length(n.vars)){
  
  rf.models[[i]] <- randomForest(recidivate ~ .,
                                 train.dat.class, ntree = 1000,
                                 mtry = n.vars[i])
  print(i)
}
save(rf.models, file = "rf_cv_models.Rdata")
load("rf_cv_models.Rdata")
n.mods <- length(rf.models)
oob.err <- rep(NA,n.mods)
n.vars <- rep(NA,n.mods)
# must use err.rate instead of mse since classification variables are factors
for (i in 1:n.mods){
  oob.err[i] <- min(rf.models[[i]]$err.rate)
  n.vars[i] <- rf.models[[i]]$mtry
}
best.mod <- which.min(oob.err)
n.vars[best.mod]
```

**Step 3: ** Predict onto test data:
```{r}
rf.yhat <- predict(rf.models[[best.mod]], newdata = test.dat.class, type = "prob")
rf.yhat <- rf.yhat[,2]
```

## Evaluating Error Metrics
### Setup
The following helper functions can be used to simplify the calculation of error metrics:
```{r}
# yhat is the vector of predicted probabilities
calc_acc <- function(y, yhat){
  yhat.class <- yhat > 0.5
  return(mean(y == yhat.class))
}
calc_prec <- function(y, yhat) {
  yhat.class <- yhat > 0.5
  return(mean(y[yhat.class == 1] == 1))
}
calc_recall <- function(y, yhat) {
  yhat.class <- yhat > 0.5
  return(mean(yhat.class[y == 1] == 1))
}
calc_mse <- function(y, yhat) {
  return(mean((yhat - y)^2))
}
calib_plot <- function(y, yhat, title) {
  # split into deciles
  prob_deciles <- seq(from = 0, to = 1, by = 0.1)
 
  # summary stats
  prob_decile_bins <- as.numeric(
  cut(yhat, breaks = prob_deciles, include.lowest = TRUE))
  
  prob_decile_bins <- prob_decile_bins/10
  
  # plot calibration
  df <- data.frame(y = y, yhat = yhat, decile_bins = prob_decile_bins)
  df <- as.data.frame(df %>%
                        group_by(decile_bins) %>%
                        summarise(actual_recidivism = mean(y),
                                  pred_prob = mean(yhat))) %>%
  gather(key = "key", value = "probability", -decile_bins)
  
  # plot
  ggplot(df, aes(x = decile_bins, y = probability, color = key)) + 
    geom_line() +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray") +
    theme_bw() + theme(plot.margin = ggplot2::margin(1.5,0,1.5,0, "cm")) +
    theme(legend.background = element_rect(fill="lightblue",
                                  size=0.4, linetype="solid", 
                                  colour ="darkblue")) +
    ggtitle(title) + xlab("Predicted Probability Decile Bin") + ylab("Probability") 
}
calib_plot_race <- function(y, yhat, race, title) {
  # split into deciles
  prob_deciles <- seq(from = 0, to = 1, by = 0.1)
 
  # summary stats
  prob_decile_bins <- as.numeric(
  cut(yhat, breaks = prob_deciles, include.lowest = TRUE))
  
  prob_decile_bins <- prob_decile_bins/10
  
  # create df
  df <- data.frame(y = y, yhat = yhat, race = race, decile_bins = prob_decile_bins)
  df <- as.data.frame(df %>%
                        group_by(race, decile_bins) %>%
                        summarise(actual_recidivism = mean(y),
                                  pred_prob = mean(yhat)))
  # plot
  ggplot(df, aes(x = decile_bins, y = actual_recidivism, color = race)) + 
    geom_line() +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray") +
    theme_bw() + theme(plot.margin = ggplot2::margin(1.5,0,1.5,0, "cm")) + 
    theme(legend.background = element_rect(fill="lightblue",
                                  size=0.4, linetype="solid", 
                                  colour ="darkblue")) +
    ggtitle(title) + 
    xlab("Pred Probability Decile Bin") + ylab("Average Recidivism Rate") 
}
```

Calculating MSE for the different models.
```{r}
mse_log <- calc_mse(dat$recidivate,log.yhat)
mse_lasso <- calc_mse(y.test,lasso.yhat)
mse_ridge <- calc_mse(y.test,rr.yhat)
mse_bt <- calc_mse(dat$recidivate,bt.yhat)
mse_rf <- calc_mse(test.dat$recidivate,rf.yhat)
```

Calculating accuracy for the different models:
```{r}
acc_log <- calc_acc(dat$recidivate,log.yhat)
acc_lasso <- calc_acc(y.test,lasso.yhat)
acc_ridge <- calc_acc(y.test,rr.yhat)
acc_bt <- calc_acc(dat$recidivate,bt.yhat)
acc_rf <- calc_acc(test.dat$recidivate,rf.yhat)
```

Calculating precision for the different models:
```{r}
prec_log <- calc_prec(dat$recidivate, log.yhat)
prec_lasso <- calc_prec(y.test, lasso.yhat)
prec_ridge <- calc_prec(y.test, rr.yhat)
prec_bt <- calc_prec(dat$recidivate, bt.yhat)
prec_rf <- calc_prec(test.dat$recidivate, rf.yhat)
```

Calculating recall for the different models:
```{r}
recall_log <- calc_recall(dat$recidivate, log.yhat)
recall_lasso <- calc_recall(y.test, lasso.yhat)
recall_ridge <- calc_recall(y.test, rr.yhat)
recall_bt <- calc_recall(test.dat$recidivate, bt.yhat)
recall_rf <- calc_recall(test.dat$recidivate, rf.yhat)
``` 

### Table of Metrics
Placing values in table:
```{r}
labcol <- c("MSE","Accuracy","Precision","Recall")
labrow <- c("Log Reg","Lasso","Ridge","Boosted Trees","Random Forest")
log_val <- c(mse_log, acc_log, prec_log, recall_log)
lasso_val <- c(mse_lasso, acc_lasso, prec_lasso, recall_lasso)
ridge_val <- c(mse_ridge, acc_ridge, prec_ridge, recall_ridge)
bt_val <- c(mse_bt, acc_bt, prec_bt, recall_bt)
rf_val <- c(mse_rf, acc_rf, prec_rf, recall_rf)
table <- matrix(c(log_val, lasso_val, ridge_val, bt_val, rf_val),nrow=5, byrow = TRUE, dimnames = list(labrow,labcol))
table
```


## Calibration Plots
Next, each model's calibration plot will better illustrate its predictive abilities:
**Logistic Regression** 
General Calibration Plot:
```{r}
calib_plot(dat$recidivate, log.yhat,"Logistic Regression Calibration Plot")
```

Calibration plot by race:
```{r}
calib_plot_race(y,log.yhat,dat$race,"Logistic Regression Calibration Plot by Race")
```

**LASSO Regression** 
General Calibration Plot:
```{r}
calib_plot(y.test,lasso.yhat,"Lasso Regression Calibration Plot")
```

Calibration plot by race:
```{r}
calib_plot_race(y.test,lasso.yhat,dat$race[-r],"Lasso Regression Calibration Plot by Race")
```

**Ridge Regression** 
General Calibration Plot:
```{r}
calib_plot(y.test,rr.yhat,"Ridge Regression Calibration Plot")
```

Calibration plot by race:
```{r}
calib_plot_race(y.test,rr.yhat,dat$race[-r],"Ridge Regression Calibration Plot by Race")
```

**Boosted Trees Model** 
General Calibration Plot:
```{r}
calib_plot(test.dat$recidivate, bt.yhat,"Boosted Trees Calibration Plot")
```

Calibration plot by race:
```{r}
calib_plot_race(test.dat$recidivate, bt.yhat, test.dat$race,"Boosted Trees Calibration Plot by Race")
```

**Random Forest Model** 
General Calibration Plot:
```{r}
calib_plot(test.dat$recidivate, rf.yhat, "Random Forest Calibration Plot")
```

Calibration plot by race:
```{r}
calib_plot_race(test.dat$recidivate, rf.yhat, test.dat$race,"Random Forest Calibration Plot by Race")
```

## Conclusion
We selected the metrics above for the following reasons:

* We factored in predictive probability into our metrics because there is a small chance this algorithm will be the decision making body. It is more likely the predicted probability of recidivism will become supplemental information for judges to decide whether or not to release an individual out on bail.
* MSE (Brier Score) was chosen over MAE because there is high consequence of a faulty prediction. Wrong predictions can lead to unnecessary detention of an otherwise innocent individual or on the other hand, endangering public safety.
* Precision is included in the analysis because we do not want to unnecessarily lock up otherwise innocent defendants.
* Recall is included in the analysis because we care about general public safety. We do not want to release dangerous individuals.
* Calibration plots provide an overarching image of how well our predicted probabilities align with recidivism rates. Breaking down the calibration plots down by race, we are able to see if an algorithm is better at predicting for some races than others. We want the performance to be fair across all races. 

By looking at the error metrics and calibration plots stored in "table", the model that consistently performs the best is **ridge regression.** It has the lowest MSE, the second highest recall, the second highest accuracy, and the third (but close) highest precision. 

Furthermore, when looking at the calibration plot, it provides the most consistent predictive accuracy across different races. 

``` {r}
final_model <- opt.model.rr
```

Reading dataset and converting necessary variables into factors:
```{r}
pseudo_dat <- read.csv("/Users/emanuelabeale/Downloads/recidivism_data_pseudo_new.csv")
pseudo_dat$race <- as.factor(pseudo_dat$race)
pseudo_dat$sex <- as.factor(pseudo_dat$sex)
pseudo_dat$charge_name <- as.factor(pseudo_dat$charge_name)
```

Applying the model to dataset:
```{r}
pseudo_dat <-  model.matrix(~.-1, pseudo_dat[,])  # Convert all categorical variable to dummies.
```

Creating prediction and classification variables:
```{r}
final_predprob <- predict(final_model, newx = pseudo_dat, type = "response")[,1]
final_class <- final_predprob > 0.5
```

